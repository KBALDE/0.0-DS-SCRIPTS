{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7y43DJkoRWMj",
    "outputId": "48d793c9-9daf-42c0-b470-6bcdc6734059"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ev0usTAThks"
   },
   "outputs": [],
   "source": [
    "#!unzip drive/MyDrive/ColabNotebooks/7.0.NOTEBOOK_UC/CV/data/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGAI2GixWeAJ"
   },
   "outputs": [],
   "source": [
    "#!pip install imageo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip kagglecatsanddogs_5340.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCat\u001b[m\u001b[m \u001b[34mDog\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls PetImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ImageClassification.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ImageClassification.py\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "# ARGS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--num_epochs\", type=int, help=\"number of epochs\")\n",
    "parser.add_argument(\"--dir_train\", type=str, help=\"the directory for training\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, help=\"learning_rate\")\n",
    "parser.add_argument(\"--image_size\", type=int, help=\"image_size\")\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, help=\"checkpoint_path\")\n",
    "parser.add_argument(\"--batch_size\", type=int, help=\"batch_size\")\n",
    "parser.add_argument(\"--data_aug\", type=bool, help=\"data_aug\")\n",
    "parser.add_argument(\"--num_classes\", type=int, help=\"num_classes\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "dir_train=args.dir_train\n",
    "num_epochs=args.num_epochs\n",
    "learning_rate=args.learning_rate\n",
    "image_size=(args.image_size, args.image_size)\n",
    "checkpoint_path=args.checkpoint_path\n",
    "batch_size=args.batch_size\n",
    "data_aug =args.data_aug\n",
    "num_classes=args.num_classes\n",
    "\n",
    "    \n",
    "# UTILS\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_data_from_dir(directory,\n",
    "                       image_size,\n",
    "                       batch_size,\n",
    "                       work_on_sample=False\n",
    "                      ):\n",
    "\n",
    "    train_dataset = image_dataset_from_directory(directory,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 image_size=image_size,\n",
    "                                                 validation_split=0.2,\n",
    "                                                 subset='training',\n",
    "                                                 seed=42)\n",
    "\n",
    "    val_dataset = image_dataset_from_directory(directory,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 image_size=image_size,\n",
    "                                                 validation_split=0.2,\n",
    "                                                 subset='validation',\n",
    "                                                 seed=42)\n",
    "    if work_on_sample:\n",
    "        train_dataset=train_dataset.take(1)\n",
    "        val_dataset= val_dataset.take(1)   \n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "    data_augmentation = tf.keras.Sequential() # define the sequential\n",
    "    data_augmentation.add(RandomFlip('horizontal'))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "    \n",
    "    return data_augmentation\n",
    "\n",
    "\n",
    "def image_classification(image_shape,\n",
    "                         base_model,\n",
    "                         data_augmentation,\n",
    "                         preprocess_input,\n",
    "                         learning_rate,\n",
    "                         num_classes,\n",
    "                         data_aug\n",
    "                        ):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "    input_shape = image_shape\n",
    "    #+ (3,)\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 120\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    if data_aug:\n",
    "        x = data_augmentation(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "    \n",
    "    # data preprocessing using the same weights the model was trained on\n",
    "    x = preprocess_input(x)\n",
    "    #x = preprocess_input(inputs) \n",
    "    \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x) \n",
    "    \n",
    "    x = tfl.GlobalAveragePooling2D()(x) \n",
    "    \n",
    "    x = tfl.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        loss_function=tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    # compile\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    metrics=['accuracy']\n",
    "\n",
    "\n",
    "    model.compile(loss = loss_function,\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "# MAIN\n",
    "def main():\n",
    "\n",
    "    train_data , val_data= read_data_from_dir(dir_train, \n",
    "                                              image_size, \n",
    "                                              batch_size, \n",
    "                                              work_on_sample=True)\n",
    "    \n",
    "    image_shape = image_size + (3,)\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
    "    data_augmentation = data_augmenter()\n",
    "    \n",
    "    model = image_classification(image_shape, \n",
    "                                 base_model, \n",
    "                                 data_augmentation,\n",
    "                                 preprocess_input,\n",
    "                                 learning_rate,\n",
    "                                 num_classes,\n",
    "                                 data_aug\n",
    "                                )\n",
    "    \n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "\n",
    "    history_fine = model.fit(train_data,\n",
    "                             epochs=num_epochs,\n",
    "                             validation_data=val_data,\n",
    "                             callbacks=[cp_callback])\n",
    "    \n",
    "    \n",
    "    print(\"I am done training\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 16:46:17.947805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Found 23410 files belonging to 2 classes.\n",
      "Using 18728 files for training.\n",
      "2023-03-16 16:46:25.345754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Found 23410 files belonging to 2 classes.\n",
      "Using 4682 files for validation.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "/Users/kindi/opt/anaconda3/lib/python3.8/site-packages/keras/backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0064 - accuracy: 0.4375\n",
      "Epoch 1: saving model to model/cp.ckpt\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.0064 - accuracy: 0.4375 - val_loss: 0.4964 - val_accuracy: 0.7188\n",
      "I am done training\n"
     ]
    }
   ],
   "source": [
    "!python ImageClassification.py --dir_train=\"PetImages\" --num_epochs=1 --batch_size=32 --learning_rate=0.001 \\\n",
    "                               --image_size=160 --checkpoint_path=\"model/cp.ckpt\" --data_aug=True --num_classes=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  cp.ckpt.index\r\n",
      "cp.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "betctqrVSXHO"
   },
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InferenceForImageClass.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InferenceForImageClass.py\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "# ARGS\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_image\", type=str, help=\"input_image\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, help=\"learning_rate\")\n",
    "parser.add_argument(\"--image_size\", type=int, help=\"image_size\")\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, help=\"checkpoint_path\")\n",
    "parser.add_argument(\"--data_aug\", type=bool, help=\"data_aug\")\n",
    "parser.add_argument(\"--num_classes\", type=int, help=\"num_classes\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "learning_rate=args.learning_rate\n",
    "image_size=(args.image_size, args.image_size)\n",
    "checkpoint_path=args.checkpoint_path\n",
    "data_aug =args.data_aug\n",
    "num_classes=args.num_classes\n",
    "input_image = args.input_image\n",
    "\n",
    "\n",
    "# UTILS\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "\n",
    "\n",
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "    data_augmentation = tf.keras.Sequential() # define the sequential\n",
    "    data_augmentation.add(RandomFlip('horizontal'))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "    \n",
    "    return data_augmentation\n",
    "\n",
    "\n",
    "def image_classification(image_shape,\n",
    "                         base_model,\n",
    "                         data_augmentation,\n",
    "                         preprocess_input,\n",
    "                         learning_rate,\n",
    "                         num_classes,\n",
    "                         data_aug\n",
    "                        ):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "    input_shape = image_shape\n",
    "    #+ (3,)\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 120\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    if data_aug:\n",
    "        x = data_augmentation(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "    \n",
    "    # data preprocessing using the same weights the model was trained on\n",
    "    x = preprocess_input(x)\n",
    "    #x = preprocess_input(inputs) \n",
    "    \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x) \n",
    "    \n",
    "    x = tfl.GlobalAveragePooling2D()(x) \n",
    "    \n",
    "    x = tfl.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        loss_function=tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    # compile\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    metrics=['accuracy']\n",
    "\n",
    "\n",
    "    model.compile(loss = loss_function,\n",
    "                  optimizer = optimizer,\n",
    "                  metrics = metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_shape = image_size + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=image_shape,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n",
    "    \n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
    "data_augmentation = data_augmenter()\n",
    "    \n",
    "model = image_classification(image_shape, \n",
    "                             base_model, \n",
    "                             data_augmentation,\n",
    "                             preprocess_input,\n",
    "                             learning_rate,\n",
    "                             num_classes,\n",
    "                             data_aug\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    img = image.load_img(input_image, target_size=image_size)\n",
    "    \n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    score = float(predictions[0])\n",
    "    if score ==1:\n",
    "        print(\"This image is a dog\")\n",
    "    else:\n",
    "        print(\"This image is a cat\")\n",
    "        \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-16 16:47:33.599666: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 16:47:36.609915: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "This image is a cat\n"
     ]
    }
   ],
   "source": [
    "!python InferenceForImageClass.py --input_image=\"PetImages/Dog/6.jpg\" --learning_rate=0.001 \\\n",
    "                               --image_size=160 --checkpoint_path=\"model/cp.ckpt\" --data_aug=False \\\n",
    "                               --num_classes=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPLOYMENT API\n",
    "\n",
    "#### FLASK API\n",
    "#### FAST API\n",
    "#### DJANGO REST API\n",
    "#### MLFLOW\n",
    "#### GRADIO\n",
    "#### STREAMLIT\n",
    "#### TENSORFLOW LIKE \n",
    "\n",
    "\n",
    "The objectif is to have a web like solution to show case the ML Model in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
